{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlgNoFvQaIlioJ9IHoQgdN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naychelynn/Notebooks_Kaggle/blob/main/BIQA_ELM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY_xKNuQhnRC"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import load_img\n",
        "from skimage.restoration import estimate_sigma\n",
        "from skimage import img_as_float\n",
        "from scipy import fftpack\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def im2patch(im, pch_size, stride=1):\n",
        "    '''\n",
        "    Reference of this part:\n",
        "    Implementation of noise level estimation of the following paper:\n",
        "    Chen G , Zhu F , Heng P A . An Efficient Statistical Method for Image Noise Level Estimation[C]// 2015 IEEE International Conference\n",
        "    on Computer Vision (ICCV). IEEE Computer Society, 2015.\n",
        "\n",
        "    Transform image to patches.\n",
        "    Input:\n",
        "        im: 3 x H x W or 1 X H x W image, numpy format\n",
        "        pch_size: (int, int) tuple or integer\n",
        "        stride: (int, int) tuple or integer\n",
        "    '''\n",
        "    if isinstance(pch_size, tuple):\n",
        "        pch_H, pch_W = pch_size\n",
        "    elif isinstance(pch_size, int):\n",
        "        pch_H = pch_W = pch_size\n",
        "    else:\n",
        "        print('The input of pch_size must be a integer or a int tuple!')\n",
        "\n",
        "    if isinstance(stride, tuple):\n",
        "        stride_H, stride_W = stride\n",
        "    elif isinstance(stride, int):\n",
        "        stride_H = stride_W = stride\n",
        "    else:\n",
        "        print('The input of stride must be a integer or a int tuple!')\n",
        "\n",
        "    stride_H, stride_W = 1 , 1\n",
        "    # print(stride_W)                                   # 3\n",
        "    C, H, W = im.shape\n",
        "    num_H = len(range(0, H-pch_H+1, stride_H))          # 169 | 377\n",
        "    num_W = len(range(0, W-pch_W+1, stride_W))          # 126 | 505\n",
        "    num_pch = num_H * num_W\n",
        "    # print(\"num_H\")\n",
        "    # print(num_H)\n",
        "    # print(\"num_W\")\n",
        "    # print(num_W)\n",
        "    pch = np.zeros((C, pch_H*pch_W, num_pch), dtype=im.dtype)\n",
        "    kk = 0\n",
        "    for ii in range(pch_H):\n",
        "        for jj in range(pch_W):\n",
        "            temp = im[:, ii:H-pch_H+ii+1:stride_H, jj:W-pch_W+jj+1:stride_W]\n",
        "            pch[:, kk, :] = temp.reshape((C, num_pch))\n",
        "            kk += 1\n",
        "\n",
        "    return pch.reshape((C, pch_H, pch_W, num_pch))\n",
        "\n",
        "\n",
        "##################################################\n",
        "\n",
        "    def noise_estimate(im, pch_size=8):\n",
        "    '''\n",
        "    Input:\n",
        "        im: the noise image, H x W x 3 or H x W numpy tensor, range [0,1]\n",
        "        pch_size: patch_size\n",
        "    Output:\n",
        "        noise_level: the estimated noise level\n",
        "    '''\n",
        "\n",
        "    if im.ndim == 3:\n",
        "        im = im.transpose((2, 0, 1))\n",
        "    else:\n",
        "        im = np.expand_dims(im, axis=0)\n",
        "\n",
        "    # image to patch\n",
        "    pch = im2patch(im, pch_size, 3)     # C x pch_size x pch_size x num_pch tensor\n",
        "\n",
        "    num_pch = pch.shape[3]              # number of patches > ..... | .....\n",
        "    pch = pch.reshape((-1, num_pch))    # d x num_pch matrix\n",
        "    # print(pch.shape)                  # (3*8*8, ..... ) > (192, ..... )\n",
        "    d = pch.shape[0]                    # 192\n",
        "    mu = pch.mean(axis=1, keepdims=True)    # d x 1 : find mean  # (192, 1)\n",
        "    X = pch - mu                        # how much varied from mean\n",
        "    sigma_X = np.matmul(X, X.transpose()) / num_pch # matrix multiplication and divide by no of patch\n",
        "    # print(sigma_X.shape)              #  (192, 192)\n",
        "    sig_value, _ = np.linalg.eigh(sigma_X) #  (192,~)\n",
        "    # sig_value.sort()                    # sort eigen value array in ascending order\n",
        "\n",
        "    for ii in range(-1, -d-1, -1):      # order array in negative sequences\n",
        "        tau = np.mean(sig_value[:ii])   # find mean of eigen values  192 .. 191 .. mean\n",
        "        # return sqrt of pixel if greater than mean or less than mean\n",
        "        if np.sum(sig_value[:ii]>tau) == np.sum(sig_value[:ii] < tau):\n",
        "            return np.sqrt(tau)"
      ],
      "metadata": {
        "id": "rI7qn-2Wh-Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    image_paths, all_est_noise_level, all_sigma_est, all_entropy_est,  = [], [], [], []\n",
        "\n",
        "    img_path = 'E:/... /01_Gaussian_TID2013/'\n",
        "    df = pd.read_csv('E:/.../'+'01_Gaussian_TID2013.csv')\n",
        "    print(\"05_Gaussian_TID2013 with Norm >>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "\n",
        "    for filename in tqdm(os.listdir(img_path)):\n",
        "        temp = filename.split(' ')\n",
        "        image_path = os.path.join(img_path, temp[0])\n",
        "        image_paths.append(image_path)\n",
        "\n",
        "\n",
        "    df['Image'] = image_paths\n",
        "    df['Score'] = df['Score'].astype(float)\n",
        "\n",
        "    for image in tqdm(df['Image']):\n",
        "        img = load_img(image, grayscale=False)\n",
        "        im = img_as_float(img)\n",
        "        start = time.time()\n",
        "\n",
        "        ####### extraction of feature 1\n",
        "        est_noise_level = noise_estimate(im, 8)\n",
        "\n",
        "        all_est_noise_level.append(est_noise_level*255)\n",
        "\n",
        "        ####### extraction of feature 2\n",
        "        # take the fourier transform of the image\n",
        "        F1 = fftpack.fft2(img)\n",
        "        # Shift the quadrants around so that low spatial frequencies are in\n",
        "        # the center of the 2D fourier transformed image.\n",
        "        F2 = fftpack.fftshift(F1)\n",
        "        # Calculate a 2D power spectrum of the Fourier Transform\n",
        "        spectrum = np.abs(F2)**2\n",
        "        # calculate spectrum entropy value from Power Spectrum\n",
        "        spectral_entropy =  stats.entropy(spectrum[:, 1])\n",
        "\n",
        "        ####### extraction of feature 3\n",
        "        # Estimate the average noise standard deviation across color channels.\n",
        "        sigma_est = estimate_sigma(im, average_sigmas=True, multichannel=True)\n",
        "\n",
        "\n",
        "        entropy_est = np.mean(spectral_entropy)         # Spectral entropy\n",
        "        all_sigma_est.append(sigma_est)\n",
        "        all_entropy_est.append(entropy_est)\n",
        "\n",
        "        end = time.time()\n",
        "        time_elapsed = end - start"
      ],
      "metadata": {
        "id": "TVvowrfIiMXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=np.vstack((np.array(all_sigma_est), np.array(all_est_noise_level),np.array(all_entropy_est)));\n",
        "irx=np.transpose(t); iry=df['Score'].tolist()\n",
        "\n",
        "pearsonr_corr_1 = stats.pearsonr(np.squeeze(all_sigma_est), np.squeeze(all_est_noise_level))\n",
        "spearmanr_corr_1 = stats.spearmanr(np.squeeze(all_sigma_est), np.squeeze(all_est_noise_level))\n",
        "print(\"\\nPLCC 1: \", pearsonr_corr_1[0], \" | SROCC 1: \", spearmanr_corr_1[0])\n",
        "all_pearsonr_corr_1.append(pearsonr_corr_1[0])\n",
        "all_spearmanr_corr_1.append(spearmanr_corr_1[0])\n",
        "\n",
        "pearsonr_corr_2 = stats.pearsonr(np.squeeze(all_est_noise_level), np.squeeze(all_entropy_est))\n",
        "spearmanr_corr_2 = stats.spearmanr(np.squeeze(all_est_noise_level), np.squeeze(all_entropy_est))\n",
        "print(\"\\nPLCC 2: \", pearsonr_corr_2[0], \" | SROCC 2: \", spearmanr_corr_2[0])\n",
        "all_pearsonr_corr_2.append(pearsonr_corr_2[0])\n",
        "all_spearmanr_corr_2.append(spearmanr_corr_2[0])\n",
        "\n",
        "pearsonr_corr_3 = stats.pearsonr(np.squeeze(all_entropy_est), np.squeeze(all_sigma_est))\n",
        "spearmanr_corr_3 = stats.spearmanr(np.squeeze(all_entropy_est), np.squeeze(all_sigma_est))\n",
        "print(\"\\nPLCC 3: \", pearsonr_corr_3[0], \" | SROCC 3: \", spearmanr_corr_3[0])\n",
        "all_pearsonr_corr_3.append(pearsonr_corr_3[0])\n",
        "all_spearmanr_corr_3.append(spearmanr_corr_3[0])\n",
        "\n",
        "#**************************\n",
        "irx, iry = np.array(irx), np.array(iry)\n",
        "all_Pear, all_Spear, all_time = [], [], []\n",
        "\n",
        "#%\n",
        "x_train, x_test, y_train, y_test = train_test_split(irx, iry, test_size=0.1)\n",
        "# build model and train\n",
        "model = elm.elm(hidden_units=500, activation_function='relu', random_type='normal', x=x_train, y=y_train, C=0.1, elm_type='reg')\n",
        "print('/n>>> L1 Regularization 500 hidden units C=0.1 Entropy >>>')\n",
        "# beta, train_accuracy, running_time = model.fit('no_re')\n",
        "beta, train_accuracy, running_time = model.fit('solution1')\n",
        "# beta, train_accuracy, running_time = model.fit('solution2')\n",
        "print('regressor running time:', running_time)\n",
        "all_time.append(running_time)"
      ],
      "metadata": {
        "id": "vBtRCXp9jMfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = ['Spatial Var', 'Fourier Entrp', 'Wavelet Std']\n",
        "forest = RandomForestRegressor(random_state=0)\n",
        "forest.fit(x_train, y_train)\n",
        "\n",
        "start_time = time.time()\n",
        "importances = forest.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
        "\n",
        "forest_importances = pd.Series(importances, index=feature_names)\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=std, ax=ax)\n",
        "plt.xticks(rotation=30)\n",
        "ax.set_title(\"Noise Statstical Feature Importance using Mean decrease in impurity\")\n",
        "ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "fig.tight_layout()\n",
        "start_time = time.time()\n",
        "result = permutation_importance(\n",
        "    forest, x_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
        ")\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time to compute the importance: {elapsed_time:.3f} seconds\")\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=None, ax=ax)\n",
        "plt.xticks(rotation=30)\n",
        "ax.set_title(\"Noise Statstical Feature Importance using Permutation\")\n",
        "ax.set_ylabel(\"Mean accuracy\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ru8lnOpYlCS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "prediction = model.predict(x_test)\n",
        "yt1, yt2 = [], []\n",
        "for i in range(len(prediction)):\n",
        "    x, y = y_test[i], prediction[i]\n",
        "    nas = np.logical_or(np.isnan(x), np.isnan(y))\n",
        "    if not nas:\n",
        "        yt1.append(x)\n",
        "        yt2.append(y)\n",
        "\n",
        "#%\n",
        "pearsonr_score = stats.pearsonr(np.squeeze(yt1), np.squeeze(yt2))\n",
        "spearmanr_score = stats.spearmanr(np.squeeze(yt1), np.squeeze(yt2))\n",
        "print(\"\\nPearson Correlation: \", pearsonr_score[0], \" | Spearman Correlation: \", spearmanr_score[0])\n",
        "all_Pear.append(pearsonr_score[0])\n",
        "all_Spear.append(spearmanr_score[0])"
      ],
      "metadata": {
        "id": "W9mXVk2flezn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}